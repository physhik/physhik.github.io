<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning on Physics to Data Science</title>
    <link>//physhik.com/categories/deep-learning/</link>
    <description>Recent content in deep learning on Physics to Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Nov 2019 01:00:00 -0700</lastBuildDate>
    
	<atom:link href="//physhik.com/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine Learns from Cardiologist (4)</title>
      <link>//physhik.com/2019/11/machine-learns-from-cardiologist-4/</link>
      <pubDate>Wed, 06 Nov 2019 01:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2019/11/machine-learns-from-cardiologist-4/</guid>
      <description>Update 
I had two emails about my ECG classifier Github repo from graduate students after I opened the source code. Please use the issue page of the repo if you have any question or an error of the code.
I myself found some errors due to the version change of Python libraries, so I updated the codes. In the near future, I would update the Python codes suitable for upgraded libraries.</description>
    </item>
    
    <item>
      <title>Fielding Independent Pitching Revisited</title>
      <link>//physhik.com/_draft/2019-10-31-sabermetrics/</link>
      <pubDate>Thu, 31 Oct 2019 01:00:00 -0700</pubDate>
      
      <guid>//physhik.com/_draft/2019-10-31-sabermetrics/</guid>
      <description>많은 사람들은 세이버메트릭션들을 숫자에 미쳐 경기를 즐기는 것을 오히려 망치는 사람들이라고 생각하는 것 같다. 실제로 야구의 인기는 줄어들고 있지 않은가
하지만 그건 구단의 최대한의 이득을 고려하는 운영방식에서의 변화 때문이지. 세이버메트릭스의 책임으로 돌리긴 어렵다
사실 세이버메트릭스는 내가 야구를 보는 관점을 넓혀줬고 오히려 더 재밌게 즐길 수 있게, 그리고 더 많은 시간을 써서 야구에 대해서 생각하게 해주었다.
또한 세이버메트릭스를 통계적인 것에만 국한하는 경우를 종종보는데, 실제로는 훨씬 넓은 의미를 가진다. 세이버메트릭스의 S가 바로 사이언스를 의미하는 것으로 야구에</description>
    </item>
    
    <item>
      <title>Parasite by Bong</title>
      <link>//physhik.com/_draft/parasite/</link>
      <pubDate>Thu, 31 Oct 2019 01:00:00 -0700</pubDate>
      
      <guid>//physhik.com/_draft/parasite/</guid>
      <description>밴쿠버에서 살면서 한국 영화를 극장에서 보긴 힘들다. 집에서부터 운전을 한시간은 해야 하고, 자전거로는 두시간 이상이 걸릴 거리에 한인타운 극장이 거의 유일한 곳이다. 기생충은 깐느 영화제에서 큰 상을 받았고 그 덕에 괜찮은 배급사와 북미 계약을 맺었다. 적지 않은 밴쿠버의 극장에서 상영 중이고, 이 영화 취향도 아닐 아내를 졸라 즐겨 찾는 멀티 플렉스 극장을 찾았다.
  Plot (spiler) from wiki 
Kim Ki-taek lives with his wife Chung-sook, son Ki-woo, and daughter Ki-jeong in a shabby semi-basement apartment.</description>
    </item>
    
    <item>
      <title>Machine Learns from Cardiologist (3)</title>
      <link>//physhik.com/2019/03/machine-learns-from-cardiologist-3/</link>
      <pubDate>Fri, 29 Mar 2019 01:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2019/03/machine-learns-from-cardiologist-3/</guid>
      <description>Open source 
The codes can be found at my Github repo. If you are familar to the models already, just see the codes. The codes are made from understanding of the research papers in Nature and the other and the open source. The host and main contributors of the linked repo are the co-authors of the original research papers. The two related research papers are easy to understand.</description>
    </item>
    
    <item>
      <title>Machine Learns from Cardiologist (2)</title>
      <link>//physhik.com/2019/03/machine-learns-from-cardiologist-2/</link>
      <pubDate>Wed, 20 Mar 2019 19:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2019/03/machine-learns-from-cardiologist-2/</guid>
      <description>Understand literatures and the result-analysis 
Deep learning and classifications. 
The pattern recognition using deep convolutional neural network is indisputably good. It shows in various complicated image recognitions or even sound recognition. It is obvious it is going to be so good at least as the similar level of human being.

What matters is if we have enough data, and how we can preprocess the data properly for machine to learn effectively.</description>
    </item>
    
    <item>
      <title>Macnine Learns from Cardiologist (1)</title>
      <link>//physhik.com/2019/03/macnine-learns-from-cardiologist-1/</link>
      <pubDate>Sun, 17 Mar 2019 20:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2019/03/macnine-learns-from-cardiologist-1/</guid>
      <description>Prologue 
Recenly the interest on wearing device is increasing, and the convolutional neural network (CNN) supervised learning must be one strong tool to analyse the signal of the body and predict the heart disease of our body.

When I scanned a few reseach papers, the 1 dimensional signal and the regular pattern of the heart beat reminds me of musical signals I researched in that it requires a signal process and neural network, and it has much potential to bring healthier life to humar races1, so I want to present the introductory post.</description>
    </item>
    
    <item>
      <title>Rough Review of WaveGAN</title>
      <link>//physhik.com/2018/02/rough-review-of-wavegan/</link>
      <pubDate>Fri, 23 Feb 2018 23:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2018/02/rough-review-of-wavegan/</guid>
      <description>Around a week ago, on ArXiv, an interesting research paper appeared, which is about the music style transfer using GAN, which is also my main topic for recent few months. Around a week ago, on arXiv, an interesting research paper appeared, which can be applied to the music style transfer using GAN, which is also my main topic for recent few months. There are already many researches on the style transfer of the images, and one of my main projects now is making the style transfer in music.</description>
    </item>
    
    <item>
      <title>Introduction to GAN </title>
      <link>//physhik.com/2017/12/introduction-to-gan/</link>
      <pubDate>Wed, 06 Dec 2017 14:00:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/12/introduction-to-gan/</guid>
      <description>I want to introduce some GAN model I have studied after I started working for the digital signal process. I will skip technical detail of the introduction. My goal is to provide a minimal background information.

Revolution in deep learning 
As we have seen at the post of VAE, generative model can be useful in machine learning. Not only one can classify the data but also can generate new data we do not have.</description>
    </item>
    
    <item>
      <title>How to Test Progressive Growing of GAN from the Github Source</title>
      <link>//physhik.com/2017/12/how-to-test-progressive-growing-of-gan-from-the-github-source/</link>
      <pubDate>Sat, 02 Dec 2017 23:50:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/12/how-to-test-progressive-growing-of-gan-from-the-github-source/</guid>
      <description>NVIDIA research team published a paper, Progressive Growing of GANs for Improved Quality, Stability, and Variation, and the source code on Github a month ago.

I went through some trials and errors to run the codes properly, so I want to make it easier to you. Why I think this post will be helpful is the Github page is not supporting to post issues to ask and answer for inquiries.</description>
    </item>
    
    <item>
      <title>Learning to Learn by Gradient Descent by Gradient Descent</title>
      <link>//physhik.com/2017/09/learning-to-learn-by-gradient-descent-by-gradient-descent/</link>
      <pubDate>Thu, 28 Sep 2017 23:50:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/09/learning-to-learn-by-gradient-descent-by-gradient-descent/</guid>
      <description>I had a trip to Quebec city for 4 days. Behind the lingering from the travel, I prepared for the meetup this week. I could not join it because of birthday dinner with my girlfriend. However, I studied the original paper seriously, and the topic involves some interesting ideas, so I want to introduce about it.

Long short term memory (LSTM) 
To understand the paper, precedently, need to understand LSTM.</description>
    </item>
    
    <item>
      <title>Neural Network (5) : Very Simple Boltzmann Machine</title>
      <link>//physhik.com/2017/09/neural-network-5-very-simple-boltzmann-machine/</link>
      <pubDate>Mon, 18 Sep 2017 23:50:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/09/neural-network-5-very-simple-boltzmann-machine/</guid>
      <description>Stochastic Hopfield net 
Boltzmann machine is nothing but stochastic Hopfield net1. If you did not yet read the post of the Hopfield net in the blog, just go read it. I assume the readers are familiar to it, and directly use many results we had in the post. The magic of deep learning which we have discussed a couple of times works here, too. Such as $\epsilon$-greedy off-policy algorithm2, the stochastic character of the binary units allows the machine occasionally increase its energy to escape from poor local minima.</description>
    </item>
    
    <item>
      <title>Neural Network (4) : Deep Reinforcement Learning, Q-learning</title>
      <link>//physhik.com/2017/09/neural-network-4-deep-reinforcement-learning-q-learning/</link>
      <pubDate>Thu, 14 Sep 2017 23:30:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/09/neural-network-4-deep-reinforcement-learning-q-learning/</guid>
      <description>Judgement Day 
It is the first time I did not post for 4 days. I was too busy to prepare for the meetup this week. The day before yesterday meetup topic was the reinforcement learning as I mentioned at previous post. It is not a long research paper, but includes 143 references. Ah, not my favorite. This A Brief Survey of Deep Reinforcement Learning did not explain the detail of what I am interested in.</description>
    </item>
    
    <item>
      <title>Neural Network (3) : Hopfield Net</title>
      <link>//physhik.com/2017/09/neural-network-3-hopfield-net/</link>
      <pubDate>Sun, 10 Sep 2017 13:30:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/09/neural-network-3-hopfield-net/</guid>
      <description>Binary Hopfield net using Hebbian learning 
We want to study Hopfield net from the simple case. Hopfield net is a fully connected feedback network. A feedback network is a network that is not a feedforward network, and in a feedforward network, all the connections are directed. All the connections in our example will be bi-directed. This symmetric property of the weight is important property of the Hopfield net.</description>
    </item>
    
    <item>
      <title>Neural Network (2) : Inference Using Perceptron and MCMC</title>
      <link>//physhik.com/2017/09/neural-network-2-inference-using-perceptron-and-mcmc/</link>
      <pubDate>Wed, 06 Sep 2017 16:30:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/09/neural-network-2-inference-using-perceptron-and-mcmc/</guid>
      <description>Single neuron still has a lot to say 
In the post of the first neural network tutorial, we studied a perceptron as a simple supervised learning machine. The perceptron is an amazing structure to understanding inference.

In the post of the first neural network tutorial, I said I would leave you to find the objective function and and draw the plot of it. I just introduce here.</description>
    </item>
    
    <item>
      <title>Neural Network (1): Perceptron and Stochastic Gradient Descent</title>
      <link>//physhik.com/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/</link>
      <pubDate>Wed, 30 Aug 2017 19:30:00 -0700</pubDate>
      
      <guid>//physhik.com/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/</guid>
      <description>Single neuron is amazing 
One of the lessons I had during physics program is that we should start to understand small thing deeply however complicated the system which you want to know is. Not just it is easier but also it helps a lot to understand the more complex ones.

Neural network is often compared to black magic. We do not understand why and how exactly so effective it is, but it makes great estimations in some specific matters.</description>
    </item>
    
  </channel>
</rss>