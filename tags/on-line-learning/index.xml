<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>on-line learning on Physics to Data Science</title>
    <link>https://physhik.github.io/tags/on-line-learning/</link>
    <description>Recent content in on-line learning on Physics to Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Sep 2017 22:30:00 -0700</lastBuildDate><atom:link href="https://physhik.github.io/tags/on-line-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MCMC (6): Gibbs Sampling and Overrelaxation</title>
      <link>https://physhik.github.io/2017/09/mcmc-6-gibbs-sampling-and-overrelaxation/</link>
      <pubDate>Fri, 01 Sep 2017 22:30:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/09/mcmc-6-gibbs-sampling-and-overrelaxation/</guid>
      <description>Efficient Monte Carlo sampling This post is on the extension of the post about Hamiltonian Monte Carlo method. Therefore, I assume the readers already read the post. Overrelaxation also reduces the random property of the Monte Carlo sampling, and speeds up the convergence of the Markov chain.
Gibbs sampling In advance of studying over relaxation, we study Gibbs sampling. In the general case of a system with K variables, a single iteration involves sampling one parameter at a time.</description>
    </item>
    
    <item>
      <title>Neural Network (1): Perceptron and Stochastic Gradient Descent</title>
      <link>https://physhik.github.io/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/</link>
      <pubDate>Wed, 30 Aug 2017 19:30:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/</guid>
      <description>Single neuron is amazing One of the lessons I had during physics program is that we should start to understand small thing deeply however complicated the system which you want to know is. Not just it is easier but also it helps a lot to understand the more complex ones.
Neural network is often compared to black magic. We do not understand why and how exactly so effective it is, but it makes great estimations in some specific matters.</description>
    </item>
    
  </channel>
</rss>
