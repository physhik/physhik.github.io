<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Markov chain Monte Carlo method on Physics to Data Science</title>
    <link>https://physhik.github.io/tags/markov-chain-monte-carlo-method/</link>
    <description>Recent content in Markov chain Monte Carlo method on Physics to Data Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Sep 2017 17:30:00 -0700</lastBuildDate><atom:link href="https://physhik.github.io/tags/markov-chain-monte-carlo-method/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MCMC (7) : Slice Sampling</title>
      <link>https://physhik.github.io/2017/09/mcmc-7-slice-sampling/</link>
      <pubDate>Tue, 05 Sep 2017 17:30:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/09/mcmc-7-slice-sampling/</guid>
      <description>Slice sampling algorithm A single transition $(x,u) \rightarrow (x&#39;,u&#39;)$ of a one-dimensional slice sampling algorithm has the following steps.
(1). evaluate $P^* (x)$
(2). draw a vertical coordinate $u&#39; \sim$ Uniform$(0,P^* (x))$
(3). create a horizontal interval $(x_l, x_r)$ enclosing $x$
 3a. draw $r \sim$ Uniform$(0,1)$
 3b. $x_l := x-rw$
 3c. $x_r := x+(1-r)w$
 3d. while $(P^* (x_l) &amp;gt; u&#39;)$ ${x_l := x-rw}$
 3e. while $(P^* (x_r) &amp;gt; u&#39;)$ ${x_r:= x+w}$</description>
    </item>
    
    <item>
      <title>MCMC (6): Gibbs Sampling and Overrelaxation</title>
      <link>https://physhik.github.io/2017/09/mcmc-6-gibbs-sampling-and-overrelaxation/</link>
      <pubDate>Fri, 01 Sep 2017 22:30:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/09/mcmc-6-gibbs-sampling-and-overrelaxation/</guid>
      <description>Efficient Monte Carlo sampling This post is on the extension of the post about Hamiltonian Monte Carlo method. Therefore, I assume the readers already read the post. Overrelaxation also reduces the random property of the Monte Carlo sampling, and speeds up the convergence of the Markov chain.
Gibbs sampling In advance of studying over relaxation, we study Gibbs sampling. In the general case of a system with K variables, a single iteration involves sampling one parameter at a time.</description>
    </item>
    
    <item>
      <title>MCMC (5) : Hamiltonian Monte Carlo Method</title>
      <link>https://physhik.github.io/2017/08/mcmc-5-hamiltonian-monte-carlo-method/</link>
      <pubDate>Fri, 25 Aug 2017 19:10:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/mcmc-5-hamiltonian-monte-carlo-method/</guid>
      <description>Yay! Finally something more directly from physics to data science. We will also have a chance to see how Metropolis-Hastings algorithm works!
The Hamiltonian Monte Carlo method is a kind of Metropolis-Hastings method. One of the weak points of Monte Carlo sampling comes up with random walks. Hamiltonian Monte Carlo method (HMC) is an approach to reducing the randomizing in algorithm of the sampling.
The original name was hybrid Monte Carlo method.</description>
    </item>
    
    <item>
      <title>MCMC (4) : Rejection Sampling</title>
      <link>https://physhik.github.io/2017/08/mcmc-4-rejection-sampling/</link>
      <pubDate>Thu, 24 Aug 2017 02:10:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/mcmc-4-rejection-sampling/</guid>
      <description>In advance, I will proceed in the extension of the previous post. I will use the same target distribution function and the similar Gaussian disposal distribution. Even Python script will be better understood if you&amp;rsquo;ve already read the previous post about importance sampling.
The rejection sampling could be the most familiar Monte Carlo sampling. When need to introduce Monte Carlo method to somebody, it is very intuitive and effective to give an example of computing the area of the circle (or anything) by using random samples.</description>
    </item>
    
    <item>
      <title>MCMC (3) : Importance Sampling</title>
      <link>https://physhik.github.io/2017/08/mcmc-3-importance-sampling/</link>
      <pubDate>Tue, 22 Aug 2017 02:10:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/mcmc-3-importance-sampling/</guid>
      <description>Importance sampling is the first sampling method I faced when I studied Monte Carlo method. Nevertheless, I haven&amp;rsquo;t seen many examples for the importance sampling. Maybe it is because the importance sampling is not effective for high dimensional systems. The weak point of the importance sampling is that the performance of it is determined by how well we choose the disposal distribution close to the target distribution.
Here, I will present a simple example of the importance sampling.</description>
    </item>
    
    <item>
      <title>MCMC (2) : Exact Monte Carlo Method</title>
      <link>https://physhik.github.io/2017/08/mcmc-2-exact-monte-carlo-method/</link>
      <pubDate>Mon, 21 Aug 2017 02:10:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/mcmc-2-exact-monte-carlo-method/</guid>
      <description>Exact Markov chain Monte Carlo sampling I don&amp;rsquo;t like the naming. The word exact could mislead us to understand the concept. Anyway I used the word in the title because it was the title of the chapter of the book &amp;ldquo;Information Theory, Inference, and Learning Algorithms&amp;rdquo; by David Mackay, which I studied to learn the theory.
The different names of it are perfect simulation and coupling from the past. Maybe these are better names.</description>
    </item>
    
    <item>
      <title>MCMC (1) : Monte Carlo Method and Metropolis-Hastings Sampling</title>
      <link>https://physhik.github.io/2017/08/mcmc-1-monte-carlo-method-and-metropolis-hastings-sampling/</link>
      <pubDate>Sun, 20 Aug 2017 14:10:00 -0700</pubDate>
      
      <guid>https://physhik.github.io/2017/08/mcmc-1-monte-carlo-method-and-metropolis-hastings-sampling/</guid>
      <description>Monte Carlo method Monte Carlo method is useful in Bayesian data modeling because maximizing posterior probability is often very difficult and fitting a Gaussian becomes hard.
Monte Carlo method becomes valuable in that we want to generate samples in some situation, and also want to estimate some expectation values of various functions.
What we deal with in this post is only small part of Monte Carlo method. It is going to be good if I have a chance to introduce about all sooner in this blog, but if not, see one of the repositories from my Github</description>
    </item>
    
  </channel>
</rss>
