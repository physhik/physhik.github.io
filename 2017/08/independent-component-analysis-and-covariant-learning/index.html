<!DOCTYPE html>
<html lang="en-us">
  <head>
    
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.59.1 with theme Tranquilpeak 0.4.3-SNAPSHOT">
<meta name="author" content="Namshik Kim">
<meta name="keywords" content=", data science, machine learning, neural network">
<meta name="description" content="Generative model 
Generative model is a model for generating all variables including outputs. I will give a very simple example with strong assumptions.

Data $\boldsymbol{x^{(n)} } $ are generated by an unknown matrix, $\boldsymbol{G}$.
$$ \boldsymbol{x} = \boldsymbol{G}~\boldsymbol{s} $$
The goal is to find the source variable $\boldsymbol{s}$.
 we assume that the number of sources is equal to the number of observations We assume that the latent variables are independently distributed, with marginal distributions We assume that the vector $\boldsymbol{x}$ is generated without noise for simplicity.">


<meta property="og:description" content="Generative model 
Generative model is a model for generating all variables including outputs. I will give a very simple example with strong assumptions.

Data $\boldsymbol{x^{(n)} } $ are generated by an unknown matrix, $\boldsymbol{G}$.
$$ \boldsymbol{x} = \boldsymbol{G}~\boldsymbol{s} $$
The goal is to find the source variable $\boldsymbol{s}$.
 we assume that the number of sources is equal to the number of observations We assume that the latent variables are independently distributed, with marginal distributions We assume that the vector $\boldsymbol{x}$ is generated without noise for simplicity.">
<meta property="og:type" content="article">
<meta property="og:title" content="Independent Component Analysis and Covariant Learning">
<meta name="twitter:title" content="Independent Component Analysis and Covariant Learning">
<meta property="og:url" content="//physhik.com/2017/08/independent-component-analysis-and-covariant-learning/">
<meta property="twitter:url" content="//physhik.com/2017/08/independent-component-analysis-and-covariant-learning/">
<meta property="og:site_name" content="Physics to Data Science">
<meta property="og:description" content="Generative model 
Generative model is a model for generating all variables including outputs. I will give a very simple example with strong assumptions.

Data $\boldsymbol{x^{(n)} } $ are generated by an unknown matrix, $\boldsymbol{G}$.
$$ \boldsymbol{x} = \boldsymbol{G}~\boldsymbol{s} $$
The goal is to find the source variable $\boldsymbol{s}$.
 we assume that the number of sources is equal to the number of observations We assume that the latent variables are independently distributed, with marginal distributions We assume that the vector $\boldsymbol{x}$ is generated without noise for simplicity.">
<meta name="twitter:description" content="Generative model 
Generative model is a model for generating all variables including outputs. I will give a very simple example with strong assumptions.

Data $\boldsymbol{x^{(n)} } $ are generated by an unknown matrix, $\boldsymbol{G}$.
$$ \boldsymbol{x} = \boldsymbol{G}~\boldsymbol{s} $$
The goal is to find the source variable $\boldsymbol{s}$.
 we assume that the number of sources is equal to the number of observations We assume that the latent variables are independently distributed, with marginal distributions We assume that the vector $\boldsymbol{x}$ is generated without noise for simplicity.">
<meta property="og:locale" content="en-us">

  
    <meta property="article:published_time" content="2017-08-29T10:30:00">
  
  
    <meta property="article:modified_time" content="2017-08-29T10:30:00">
  
  
  
    
      <meta property="article:section" content="ML">
    
      <meta property="article:section" content="maximum likelihood">
    
      <meta property="article:section" content="independent component anlysis">
    
  
  
    
      <meta property="article:tag" content="optimization">
    
      <meta property="article:tag" content="covariant equation">
    
      <meta property="article:tag" content="generative model">
    
      <meta property="article:tag" content="independent component analysis">
    
  


<meta name="twitter:card" content="summary">







  <meta property="og:image" content="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_20_0.png">
  <meta property="twitter:image" content="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_20_0.png">





  <meta property="og:image" content="/images/avatar.jpg">
  <meta property="twitter:image" content="/images/avatar.jpg">


    <title>Independent Component Analysis and Covariant Learning</title>

    <link rel="icon" href="//physhik.com/favicon.ico">
    

    

    <link rel="canonical" href="//physhik.com/2017/08/independent-component-analysis-and-covariant-learning/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/jquery.fancybox.min.css" integrity="sha256-vuXZ9LGmmwtjqFX1F+EKin1ThZMub58gKULUyf0qECk=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.4/helpers/jquery.fancybox-thumbs.min.css" integrity="sha256-SEa4XYAHihTcEP1f5gARTB2K26Uk8PsndQYHQC1f4jU=" crossorigin="anonymous" />
    
    
    <link rel="stylesheet" href="//physhik.com/css/style-nnm2spxvve8onlujjlegkkytaehyadd4ksxc1hyzzq9a2wvtrgbljqyulomn.min.css" />
    
    

    
      
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-83159020-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="//physhik.com/">Physics to Data Science</a>
  </div>
  
    
      <a class="header-right-picture "
         href="//physhik.com/#about">
    
    
    
      
        <img class="header-picture" src="//physhik.com/images/avatar.jpg" alt="Author&#39;s picture" />
      
    
    </a>
  
  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="//physhik.com/#about">
          <img class="sidebar-profile-picture" src="//physhik.com/images/avatar.jpg" alt="Author&#39;s picture" />
        </a>
        <h4 class="sidebar-profile-name">Namshik Kim</h4>
        
          <h5 class="sidebar-profile-bio">physicist, data scientist</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="//physhik.com/">
    
      <i class="sidebar-button-icon fa fa-lg fa-home"></i>
      
      <span class="sidebar-button-desc">Home</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="//physhik.com/categories">
    
      <i class="sidebar-button-icon fa fa-lg fa-bookmark"></i>
      
      <span class="sidebar-button-desc">Categories</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="//physhik.com/tags">
    
      <i class="sidebar-button-icon fa fa-lg fa-tags"></i>
      
      <span class="sidebar-button-desc">Tags</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="//physhik.com/archives">
    
      <i class="sidebar-button-icon fa fa-lg fa-archive"></i>
      
      <span class="sidebar-button-desc">Archives</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="//physhik.com/about/index.html">
    
      <i class="sidebar-button-icon fa fa-lg fa-question"></i>
      
      <span class="sidebar-button-desc">About</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/physhik" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-github"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.linkedin.com/in/namshikkim/" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-linkedin"></i>
      
      <span class="sidebar-button-desc">LinkedIn</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://www.researchgate.net/profile/Namshik_Kim" target="_blank" rel="noopener">
    
      <i class="sidebar-button-icon fa fa-lg fa-research-gate"></i>
      
      <span class="sidebar-button-desc">ResearchGate</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      

    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" itemscope itemType="//schema.org/BlogPosting">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title" itemprop="headline">
      Independent Component Analysis and Covariant Learning
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time itemprop="datePublished" datetime="2017-08-29T10:30:00-07:00">
        
  August 29, 2017

      </time>
    
    
  
  
    <span>in</span>
    
      <a class="category-link" href="//physhik.com/categories/ml">ML</a>, 
    
      <a class="category-link" href="//physhik.com/categories/maximum-likelihood">maximum likelihood</a>, 
    
      <a class="category-link" href="//physhik.com/categories/independent-component-anlysis">independent component anlysis</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown" itemprop="articleBody">
            <div class="main-content-wrap">
              

<p><br></p>

<h2 id="generative-model">Generative model</h2>

<p><br></p>

<p>Generative model is a model for generating all variables including outputs. I will give a very simple example with strong assumptions.</p>

<p><br></p>

<p>Data $\boldsymbol{x^{(n)} } $ are generated by an unknown matrix, $\boldsymbol{G}$.</p>

<p>$$
\boldsymbol{x} = \boldsymbol{G}~\boldsymbol{s}
$$</p>

<p>The goal is to find the source variable $\boldsymbol{s}$.</p>

<ul>
<li>we assume that the number of sources is equal to the number of observations</li>
<li>We assume that the latent variables are independently distributed, with marginal distributions</li>
<li>We assume that the vector $\boldsymbol{x}$ is generated without noise for simplicity.</li>
</ul>

<p><br></p>

<p>$$
z_i = \phi_i(s_i)
$$</p>

<p>and</p>

<p>$$
\boldsymbol{W}^{-1} = \boldsymbol{G}
$$</p>

<p>and</p>

<p>$$
\phi_i(s_i) \equiv {d\ln~p_i(s_i) \over ds_i}.
$$</p>

<p>$\phi_i(s_i)$ can be chosen appropriately for the proper probability density. For example, $\phi_i(s_i) = -\tanh(s_i)$  or  $CauchyPDF(s_i)$</p>

<p><br></p>

<h3 id="contour-plots-from-the-generative-model">Contour plots from the generative model</h3>

<p><br></p>

<pre><code class="language-python">import random
import numpy as np

import scipy.stats as st
import matplotlib.pyplot as plt
import scipy.special as sp
import scipy.integrate as integrate
%precision 6
from __future__ import division

%matplotlib inline
plt.style.use('ggplot')
</code></pre>

<p>Set the matrix and check the matrix multiplications set correctly.</p>

<p><br></p>

<pre><code class="language-python">G = np.array([[3/4, 1/2],[1/2, 1]])
G2 = np.array([[2, -1],[-1, 3/2]])
</code></pre>

<pre><code class="language-python">W = np.linalg.inv(G)
W2 = np.linalg.inv(G2)
</code></pre>

<pre><code class="language-python">ax , ay = np.dot(W, [x, y])
ax2 , ay2 = np.dot(W2, [x, y])
</code></pre>

<pre><code class="language-python">zx, zy  = -np.tanh(ax), -np.tanh(ay)
zx2, zy2  = -np.tanh(ax2), -np.tanh(ay2)
</code></pre>

<pre><code class="language-python">px, py = 1/np.cosh(ax), 1/np.cosh(ay)
px2, py2 = 1/np.cosh(ax2), 1/np.cosh(ay2)
</code></pre>

<p>Distributions over two observables generated by $1/\cosh$ distributions on the latent variables, for $G$ and $G2$ ( or $W$ and $W2$).</p>

<p><br></p>

<pre><code class="language-python">
N = 100
x, y = np.linspace(-4,4, N), np.linspace(-4,4, N)
X, Y = np.meshgrid(x, y)
Z , Z2 = np.zeros((N,N)), np.zeros((N,N))
for i in range(N):
    for j in range(N):
         Z[i,j]=1/np.cosh(np.dot(W, [X[i,j], Y[i,j]]))[0]*1/np.cosh(np.dot(W, [X[i,j], Y[i,j]]))[1]
for i in range(N):
    for j in range(N):
         Z2[i,j]=1/np.cosh(np.dot(W2, [X[i,j], Y[i,j]]))[0]*1/np.cosh(np.dot(W2, [X[i,j], Y[i,j]]))[1]        
plt.contour(Z)
plt.contour(Z2)
plt.colorbar()
plt.show()
</code></pre>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_8_0.png" alt="png" /></p>

<p><br></p>

<p>From the hyper-cosine distribution, it is hard to see the effect of generative matrix, $G$. It has heavier tail than Gaussian but still quite center oriented. Let us try Cauchy distribution. See the contours of the generative distributions when the latent variables have Cauchy distributions.</p>

<p><br></p>

<pre><code class="language-python">
N = 100
x, y = np.linspace(-2,2, N), np.linspace(-2,2, N)
X, Y = np.meshgrid(x, y)
Z, Z2 = np.zeros((N,N)), np.zeros((N,N))
for i in range(N):
    for j in range(N):
         Z[i,j]=Cauchy(np.dot(W, [X[i,j], Y[i,j]]))[0]*Cauchy(np.dot(W, [X[i,j], Y[i,j]]))[1]

for i in range(N):
    for j in range(N):
         Z2[i,j]=Cauchy(np.dot(W2, [X[i,j], Y[i,j]]))[0]*Cauchy(np.dot(W2, [X[i,j], Y[i,j]]))[1]

plt.contour(Z)
plt.contour(Z2)
plt.colorbar()
plt.show()

</code></pre>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_11_0.png" alt="png" /></p>

<p><br></p>

<p>Still the effect is small. I want to see the tail from the Cauchy distribution. To set the proper level of contour, check the maximum and minimum of the function.</p>

<p><br></p>

<pre><code class="language-python">Z.min(), Z.max()
</code></pre>

<pre><code>(0.000000, 9.123957)
</code></pre>

<p>The maximum of the probability density is around 9.1 because I did not normalize the density distribution.</p>

<p><br></p>

<pre><code class="language-python">
N = 100
x, y = np.linspace(-8,8, N), np.linspace(-8,8, N)
X, Y = np.meshgrid(x, y)
Z, Z2 = np.zeros((N,N)), np.zeros((N,N))
for i in range(N):
    for j in range(N):
         Z[i,j]=Cauchy(np.dot(W, [X[i,j], Y[i,j]]))[0]*Cauchy(np.dot(W, [X[i,j], Y[i,j]]))[1]

for i in range(N):
    for j in range(N):
         Z2[i,j]=Cauchy(np.dot(W2, [X[i,j], Y[i,j]]))[0]*Cauchy(np.dot(W2, [X[i,j], Y[i,j]]))[1]

contour_levels = np.arange(0.01, 0.3, 0.01)
plt.contour(Z, contour_levels )
plt.contour(Z2, contour_levels )
plt.colorbar()


</code></pre>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_13_1.png" alt="png" /></p>

<p>Now it looks better.</p>

<p><br></p>

<h2 id="learning-algorithm-and-maximum-likelihood-again-1">Learning algorithm and maximum likelihood again<sup class="footnote-ref" id="fnref:1"><a href="#fn:1">1</a></sup></h2>

<p><br></p>

<p>As I mentioned earlier, the generative matrix, $G$, is unknown. If our initial matrix was improper, we can run the learning algorithm for optimization. However, we can&rsquo;t arbitrarily change the matrix.</p>

<p><br></p>

<p>For learning about $\boldsymbol{G}$, investigate the likelihood,</p>

<p>$P(\boldsymbol{x}^{(n)} | \boldsymbol{G}, \mathcal{H})$.</p>

<p><br></p>

<p>$$
\ln~P(\boldsymbol{x}^{(n)}|\boldsymbol{G}, \mathcal{H}) = -\ln ~|\det ~\boldsymbol{G}|+\sum_i \ln~p_i(G^{-1}_{ij}x_j)
$$</p>

<p>Take the derivative of the logarithm likelihood, then we obtain</p>

<p>$$
{\partial \over \partial W_{ij}}\ln~P(\boldsymbol{x}^{(n)}|\boldsymbol{G}, \mathcal{H})  = G_{ij }+x_jz_i
$$</p>

<p><br></p>

<p>Induce of equation (6) may be found in <a href="//physhik.com/images/postimages/postimages/proofofequations.zip">this link</a><sup class="footnote-ref" id="fnref:2"><a href="#fn:2">2</a></sup>, the proof of the identities of the book, (34.10)-(34.12) are also linked. These identities are very useful to take the derivative with respect to $W<em>{ij}$ or $G</em>{ij}$.</p>

<p><br></p>

<p>Now for the Python coding example, I will make contour plots by learning.</p>

<p>$$
\Delta \boldsymbol{W} = \epsilon ( [\boldsymbol{W^{T}}]^{-1}+\boldsymbol{zx^T})
$$</p>

<p>The epsilon is chosen by hand here since we are not really solving the problem by the real data. I put only one generative matrix for each graph to see more clearly.</p>

<pre><code class="language-python">plt.hold(False)
N = 100
x, y = np.linspace(-4,4, N), np.linspace(-4,4, N)
X, Y = np.meshgrid(x, y)
Z  = np.zeros((N,N))

G = np.array([[3/4, 1/2],[1/2, 1]])
W = np.linalg.inv(G)

turns = 0
epsilon = 0.0001
WN = W
while turns &lt;5:


    Z  = np.zeros((N,N))

    for i in range(N):
        for j in range(N):
            Z[i,j]=1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[0]*1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[1]

    plt.contour(Z, np.array([0.035]))
    plt.hold(True)
    dW = np.linalg.inv(np.transpose(WN))+np.dot([-np.tanh(np.dot(WN, [x, y]))], np.transpose([x,y]))


    WN= WN+epsilon *dW[0]

    turns +=1

plt.show()
</code></pre>

<p><br>
One contour plot for each turn (same blue plots)
<br></p>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_17_0.png" alt="png" /></p>

<pre><code class="language-python">plt.hold(False)
N = 100
x, y = np.linspace(-4,4, N), np.linspace(-4,4, N)
X, Y = np.meshgrid(x, y)
Z  = np.zeros((N,N))

G = np.array([[3/4, 1/2],[1/2, 1]])
W = np.linalg.inv(G)

turns = 0
epsilon = 0.0001
WN = W
while turns &lt;5:


    Z  = np.zeros((N,N))

    for i in range(N):
        for j in range(N):
            Z[i,j]=1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[0]*1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[1]

    plt.contour(Z)
    plt.hold(True)
    dW = np.linalg.inv(np.transpose(WN))+np.dot([-np.tanh(np.dot(WN, [x, y]))], np.transpose([x,y]))


    WN= WN+epsilon *dW[0]

    turns +=1
plt.colorbar()
plt.show()
</code></pre>

<p><br>
See the learnings for many contours.
<br></p>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_18_0.png" alt="png" /></p>

<pre><code class="language-python">plt.hold(False)
N = 100
x, y = np.linspace(-4,4, N), np.linspace(-4,4, N)
X, Y = np.meshgrid(x, y)
Z  = np.zeros((N,N))

G = np.array([[3/4, 1/2],[1/2, 1]])
W = np.linalg.inv(G)

turns = 0
epsilon = 0.0002
WN = W
while turns &lt;10:


    Z  = np.zeros((N,N))

    for i in range(N):
        for j in range(N):
            Z[i,j]=Cauchy(np.dot(WN, [X[i,j], Y[i,j]]))[0]*Cauchy(np.dot(WN, [X[i,j], Y[i,j]]))[1]


    plt.contour(Z, np.array([0.35]))
    plt.hold(True)
    dW = np.linalg.inv(np.transpose(WN))+np.dot([-np.tanh(np.dot(WN, [x, y]))], np.transpose([x,y]))


    WN= WN+epsilon *dW[0]

    turns +=1

plt.show()
</code></pre>

<p><br>
For Cauchy distribution
<br></p>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_19_0.png" alt="png" /></p>

<p><br></p>

<p>Thus, we carry out that the distribution is getting broader in the specific direction.</p>

<p><br></p>

<h2 id="covariant-gradient-learning-algorithm">Covariant gradient learning algorithm</h2>

<p><br></p>

<h3 id="covaiance-and-tensors">Covaiance and tensors</h3>

<p><br>
The name of the section 34.3 in the <a href="https://inference.phy.cam.ac.uk/mackay/itila/book.html">the book</a>, <em>Information Theory, Inference, and Learning Algorithms</em>, is <em>&ldquo;A covariant, simpler, and faster learning algorithm&rdquo;</em>. Nevertheless, I could not find any further explanation about the meaning of covariance here. It is quite confusing because we use the terminology <em>variance</em> to describe the distributions. The covariance in statistics is defined to exhibit how much two values are linearly related to each other. $Cov(f(x),g(x))=E[(f(x)-E[f(x)])(g(x)-E[g(x)])]$ where $E(f)$ is the expectation of a function, $f$.</p>

<p><br></p>

<p>Here, the covariance is absolutely <strong>different</strong> thing. I love this book, but sometimes I feel this book is not much self-contained for engineers. The concept is significantly studied in differential geometry or theory of gravitation and high energy physics such as quantum field theory and string theory. Thus, I feel very comfortable to handle the tensorial expressions in differential geometry present in machine learning.</p>

<p><br></p>

<p>Simply put, tensor is a high dimensional matrix or extended component quantities. Matrix is a mere rank 2-tensor. Rank 3 tensor looks like this. $T_{ijk}$ and</p>

<p>$T_{jk}^i$ and so on.</p>

<p><br></p>

<p>Why do we, all of a sudden, talk about geometry or general relativity of Einstein? See the equation (6) again. The derivative was taken with respect to matrix, $W_{ij}$. Then, we should make the equation tensorial, which follows the transformation rules of tensors.</p>

<p><br></p>

<p>The book mentioned about metric and curvature. Both are very well-known tensors. Let&rsquo;s consider two paths from the same starts to same ends, from $(0,~0)$ to $(1,~1)$. Two observers move along the different paths and measure the time takes to go to the goal. In the flat world, it takes same time. Both movements are just equivalent with different orders. However, imagine something there was hills<sup class="footnote-ref" id="fnref:3"><a href="#fn:3">3</a></sup> for one path. Then it would take more time because the steep uphill is fatal. In other words, because of the curved spacetime, the path was not commutable, and the quantities to exhibit the commutator is a curvature.</p>

<p><br></p>

<p>Metric tensor is the most famous rank 2 tensor, $g_{ij}$, which has two components. It appears when we define intervals.</p>

<p>$$
ds^2 = g_{ij}dx^idx^j
$$</p>

<p><br></p>

<p>I discern upper index and low index of the vector and tensor. In cartesian spacetime, the metric tensor is an identity matrix, so $x^i$ and $x_i$ are identical, but if the metric tensor has off-diagonal terms, we should distinguish them. The metric tensor indeed plays a role to lower or raise the indices.</p>

<p><br></p>

<p>$$
\partial_i \equiv {\partial \over \partial x^i},~~ and ~~  \partial^i \equiv {\partial \over \partial x_i}
$$</p>

<p><br></p>

<p>As I commented the curvature tensor emerges from the non-commutivity, the non-diagonalized matrix derivative causes a curvature.</p>

<p><br></p>

<p>$$
R^{(ij)(kl)} \equiv {\partial \over \partial W_{ij}}{\partial \over \partial W_{kl}} - {\partial \over \partial W_{kl}}{\partial \over \partial W_{ij}} \ne 0
$$</p>

<p>The above rank 4-tensor is such as a curvature tensor<sup class="footnote-ref" id="fnref:4"><a href="#fn:4">4</a></sup>.</p>

<p><br></p>

<p>In contrast, the curvature in cartesian flat space is none. In this case, we do not need this covariantize steps at all.</p>

<p>$$
\partial_i \partial_j - \partial_j \partial_i = 0
$$</p>

<p><br></p>

<h3 id="covariant-learning-algorithm">Covariant learning algorithm</h3>

<p><br></p>

<p>Thus, I rewrite the equation (34.19) of the book with the tensor convention.</p>

<p>$$
\Delta \omega_{i} = \epsilon {\partial L \over \partial \omega_i}
$$</p>

<p>This is not covariant in terms of index, too. LHS is low index vector, and RHS is upper index vector because low index derivative is taken as upper index vector. To make the RHS upper index vector, we should multiply a rank 2 tensor with two low indices such as $M_{ij}$. That is the equation (34.20) of the book.</p>

<p><br></p>

<p>In our case, we take the derivative with respect to $W_{ij}$,</p>

<p>and then we need rank 4-tensor such as $M_{ijkl}$. The most famous rank 4-tensor is a curvature tensor. By multiplying a curvature tensor obtained, we induce the covariant learning algorithm.</p>

<p><br></p>

<p>The following learning was made with the covariant algorithm.</p>

<p><br></p>

<pre><code class="language-python"># covaraint

plt.hold(False)
N = 100
x, y = np.linspace(-4,4, N), np.linspace(-4,4, N)
X, Y = np.meshgrid(x, y)
Z  = np.zeros((N,N))

G = np.array([[3/4, 1/2],[1/2, 1]])
W = np.linalg.inv(G)

turns = 0
epsilon = 0.0001
WN = W
while turns &lt;10:


    Z  = np.zeros((N,N))

    for i in range(N):
        for j in range(N):
            Z[i,j]=1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[0]*1/3.14141*1/np.cosh(np.dot(WN, [X[i,j], Y[i,j]]))[1]

    plt.contour(Z)
    plt.hold(True)
    dW = WN+np.dot([-np.tanh(np.dot(WN, [x, y]))], np.transpose(np.dot(np.dot(np.transpose(WN), WN),[x,y])))


    WN= WN+epsilon *dW[0]

    turns +=1
plt.colorbar()
plt.show()
</code></pre>

<p><img src="//physhik.com/images/postimages/independentCompAnalysis_files/independentCompAnalysis_20_0.png" alt="png" /></p>

<p><br></p>
<div class="footnotes">

<hr />

<ol>
<li id="fn:1">Again? Did I post about maximum likelihood? Well, I did not. I planned to discuss about it in soft K-means clustering. We will be back to the clustering and handle how maximum likelihood leads us to better clustering, someday.
 <a class="footnote-return" href="#fnref:1"><sup>[return]</sup></a></li>
<li id="fn:2">I would create pdf documents in LateX someday, too. Now, please be contented by the ugly handwriting in photos.
 <a class="footnote-return" href="#fnref:2"><sup>[return]</sup></a></li>
<li id="fn:3">For example, uphill to $(1,~0)$ and downhill to $(1,~1)$.
 <a class="footnote-return" href="#fnref:3"><sup>[return]</sup></a></li>
<li id="fn:4">The tensor is some special case of the Ricci tensor.
 <a class="footnote-return" href="#fnref:4"><sup>[return]</sup></a></li>
</ol>
</div>

              
            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">TAGGED IN</span><br/>
                    
  <a class="tag tag--primary tag--small" href="//physhik.com/tags/optimization/">optimization</a>

  <a class="tag tag--primary tag--small" href="//physhik.com/tags/covariant-equation/">covariant equation</a>

  <a class="tag tag--primary tag--small" href="//physhik.com/tags/generative-model/">generative model</a>

  <a class="tag tag--primary tag--small" href="//physhik.com/tags/independent-component-analysis/">independent component analysis</a>

                  </div>
                
              
            
            <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="//physhik.com/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/" data-tooltip="Neural Network (1): Perceptron and Stochastic Gradient Descent">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="//physhik.com/2017/08/variational-method-for-optimization/" data-tooltip="Variational Method for Optimization">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

            
              
                <div id="disqus_thread">
  <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
              
            
          </div>
        </article>
        <footer id="footer" class="main-content-wrap">
  <span class="copyrights">
    &copy; 2019 Namshik Kim. All Rights Reserved
  </span>
</footer>

      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        <div class="post-actions-wrap">
  
      <nav >
        <ul class="post-actions post-action-nav">
          
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="//physhik.com/2017/08/neural-network-1-perceptron-and-stochastic-gradient-descent/" data-tooltip="Neural Network (1): Perceptron and Stochastic Gradient Descent">
              
                  <i class="fa fa-angle-left"></i>
                  <span class="hide-xs hide-sm text-small icon-ml">NEXT</span>
                </a>
            </li>
            <li class="post-action">
              
                <a class="post-action-btn btn btn--default tooltip--top" href="//physhik.com/2017/08/variational-method-for-optimization/" data-tooltip="Variational Method for Optimization">
              
                  <span class="hide-xs hide-sm text-small icon-mr">PREVIOUS</span>
                  <i class="fa fa-angle-right"></i>
                </a>
            </li>
          
        </ul>
      </nav>
    <ul class="post-actions post-action-share" >
      
        <li class="post-action hide-lg hide-md hide-sm">
          <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions">
            <i class="fa fa-share-alt"></i>
          </a>
        </li>
        
      
      
        <li class="post-action">
          <a class="post-action-btn btn btn--default" href="#disqus_thread">
            <i class="fa fa-comment-o"></i>
          </a>
        </li>
      
      <li class="post-action">
        
          <a class="post-action-btn btn btn--default" href="#">
        
          <i class="fa fa-list"></i>
        </a>
      </li>
    </ul>
  
</div>

      </div>
      <div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-close"></i>
  <ul class="share-options">
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>
    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-remove"></i>
    </div>
    
      <img id="about-card-picture" src="//physhik.com/images/avatar.jpg" alt="Author&#39;s picture" />
    
    <h4 id="about-card-name">Namshik Kim</h4>
    
      <div id="about-card-bio">physicist, data scientist</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Data Scientist
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker"></i>
        <br/>
        Vancouver, BC, Canada.
      </div>
    
  </div>
</div>

    <div id="algolia-search-modal" class="modal-container">
  <div class="modal">
    <div class="modal-header">
      <span class="close-button"><i class="fa fa-close"></i></span>
      <a href="https://algolia.com" target="_blank" rel="noopener" class="searchby-algolia text-color-light link-unstyled">
        <span class="searchby-algolia-text text-color-light text-small">by</span>
        <img class="searchby-algolia-logo" src="https://www.algolia.com/static_assets/images/press/downloads/algolia-light.svg">
      </a>
      <i class="search-icon fa fa-search"></i>
      <form id="algolia-search-form">
        <input type="text" id="algolia-search-input" name="search"
          class="form-control input--large search-input" placeholder="Search" />
      </form>
    </div>
    <div class="modal-body">
      <div class="no-result text-color-light text-center">no post found</div>
      <div class="results">
        
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/11/machine-learns-from-cardiologist-4/">
                <h3 class="media-heading">Machine Learns from Cardiologist (4)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Nov 11, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Update 
I had two emails about my ECG classifier Github repo from graduate students after I opened the source code. Please use the issue page of the repo if you have any question or an error of the code.
I myself found some errors due to the version change of Python libraries, so I updated the codes. In the near future, I would update the Python codes suitable for upgraded libraries.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/10/new-blog-theme/">
                <h3 class="media-heading">New Blog Theme</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Oct 10, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">I decided to use my own domain instead of renting the /github.io/, and also to insert Google adsense in my blog if possible. Even if I updated my blog only 10 times since Oct, 2017, the number of visitors and their sessions were steady by Google analysis. I appreicate the interest on my posts. Recently I started updating my blog again, and want to see the more industrial analytic result. At least I am sure the profit from the adsense will cover the cost for the domain.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/03/machine-learns-from-cardiologist-3/">
                <h3 class="media-heading">Machine Learns from Cardiologist (3)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Open source 
The codes can be found at my Github repo. If you are familar to the models already, just see the codes. The codes are made from understanding of the research papers in Nature and the other and the open source. The host and main contributors of the linked repo are the co-authors of the original research papers. The two related research papers are easy to understand.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/03/machine-learns-from-cardiologist-2/">
                <h3 class="media-heading">Machine Learns from Cardiologist (2)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Understand literatures and the result-analysis 
Deep learning and classifications. 
The pattern recognition using deep convolutional neural network is indisputably good. It shows in various complicated image recognitions or even sound recognition. It is obvious it is going to be so good at least as the similar level of human being.

What matters is if we have enough data, and how we can preprocess the data properly for machine to learn effectively.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/03/macnine-learns-from-cardiologist-1/">
                <h3 class="media-heading">Macnine Learns from Cardiologist (1)</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Mar 3, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Prologue 
Recenly the interest on wearing device is increasing, and the convolutional neural network (CNN) supervised learning must be one strong tool to analyse the signal of the body and predict the heart disease of our body.

When I scanned a few reseach papers, the 1 dimensional signal and the regular pattern of the heart beat reminds me of musical signals I researched in that it requires a signal process and neural network, and it has much potential to bring healthier life to humar races1, so I want to present the introductory post.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/02/youtube-data-api-on-gcp/">
                <h3 class="media-heading">Youtube Data API on GCP</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">To architect low cost and well-performing server, many companies use cloud service such as Amazon AWS, Google clound platform (GCP). I have used AWS EC2 with GPU and S3 storage for my deep learning research at Soundcorset.

AWS and GCP opened many cloud platform services, and to build the data pipeline and to manage the data effectively, need to learn the command line tool and API. In this post, I will discuss the Google Youtube data API because recently I studied.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2019/02/clean-coding-and-short-run-time/">
                <h3 class="media-heading">Clean Coding and Short Run Time</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2019
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Today I want to discuss purely about coding itself. I wish this post is helpful for someone want to transit his career from a pure researcher to a programmer. I have been a researcher rather than a programmer. I would just want to execute something to see the result I wanted to see. If the run time is too long or my computer has no enough memory to run the code, it was a sign of new purchase to me.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2018/02/revisited-variational-inference/">
                <h3 class="media-heading">Revisited Variational Inference</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">A few days ago, I was asked what the variational method is, and I found my previous post, Variational Method for Optimization, barely explain some basic of variational method. Thus, I would do it in this post.

Data concerned in machine learning are ruled by physics of informations. It sounds quite abstract, so I will present an example of dynamic mechanics. Let us consider a ball thrown with velocity v=($v_x$, $v_y$) at x = (x, y), and under the vertical gravity with constant g.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2018/02/rough-review-of-wavegan/">
                <h3 class="media-heading">Rough Review of WaveGAN</h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Feb 2, 2018
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">Around a week ago, on ArXiv, an interesting research paper appeared, which is about the music style transfer using GAN, which is also my main topic for recent few months. Around a week ago, on arXiv, an interesting research paper appeared, which can be applied to the music style transfer using GAN, which is also my main topic for recent few months. There are already many researches on the style transfer of the images, and one of my main projects now is making the style transfer in music.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
          <div class="media">
            
            <div class="media-body">
              <a class="link-unstyled" href="//physhik.com/2017/12/introduction-to-gan/">
                <h3 class="media-heading">Introduction to GAN </h3>
              </a>
              <span class="media-meta">
                <span class="media-date text-small">
                  Dec 12, 2017
                </span>
              </span>
              <div class="media-content hide-xs font-merryweather">I want to introduce some GAN model I have studied after I started working for the digital signal process. I will skip technical detail of the introduction. My goal is to provide a minimal background information.

Revolution in deep learning 
As we have seen at the post of VAE, generative model can be useful in machine learning. Not only one can classify the data but also can generate new data we do not have.</div>
            </div>
            <div style="clear:both;"></div>
            <hr>
          </div>
        
      </div>
    </div>
    <div class="modal-footer">
      <p class="results-count text-medium"
         data-message-zero="no post found"
         data-message-one="1 post found"
         data-message-other="{n} posts found">
         36 posts found
      </p>
    </div>
  </div>
</div>
    
  
    
    <div id="cover" style="background-image:url('//physhik.com/images/cover.jpg');"></div>
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.2.4/jquery.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/2.1.7/js/jquery.fancybox.min.js" integrity="sha256-GEAnjcTqVP+vBp3SSc8bEDQqvWAZMiHyUSIorrWwH50=" crossorigin="anonymous"></script>


<script src="//physhik.com/js/script-qi9wbxp2ya2j6p7wx1i6tgavftewndznf4v0hy2gvivk1rxgc3lm7njqb6bz.min.js"></script>


<script lang="javascript">
window.onload = updateMinWidth;
window.onresize = updateMinWidth;
document.getElementById("sidebar").addEventListener("transitionend", updateMinWidth);
function updateMinWidth() {
  var sidebar = document.getElementById("sidebar");
  var main = document.getElementById("main");
  main.style.minWidth = "";
  var w1 = getComputedStyle(main).getPropertyValue("min-width");
  var w2 = getComputedStyle(sidebar).getPropertyValue("width");
  var w3 = getComputedStyle(sidebar).getPropertyValue("left");
  main.style.minWidth = `calc(${w1} - ${w2} - ${w3})`;
}
</script>

<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>


  
    
      <script>
        var disqus_config = function () {
          this.page.url = '\/\/physhik.com\/2017\/08\/independent-component-analysis-and-covariant-learning\/';
          
            this.page.identifier = '\/2017\/08\/independent-component-analysis-and-covariant-learning\/'
          
        };
        (function() {
          
          
          if (window.location.hostname == "localhost") {
            return;
          }
          var d = document, s = d.createElement('script');
          var disqus_shortname = 'physhiks-data-science';
          s.src = '//' + disqus_shortname + '.disqus.com/embed.js';

          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
        })();
      </script>
    
  


  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      CommonHTML: { linebreaks: { automatic: true } },
      tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
      messageStyle: 'none'
    });
  </script>



    
  </body>
</html>

